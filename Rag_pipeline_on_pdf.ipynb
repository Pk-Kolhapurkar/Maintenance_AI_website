{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3--WhrahKL2",
        "outputId": "18f8b5d1-0e40-4317-dd39-b970e346684b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/ibm-granite-community/utils.git\n",
            "  Cloning https://github.com/ibm-granite-community/utils.git to /tmp/pip-req-build-ri_v791d\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/ibm-granite-community/utils.git /tmp/pip-req-build-ri_v791d\n",
            "  Resolved https://github.com/ibm-granite-community/utils.git to commit f5bbcb9dbd203d3a425a64d6af01bf8af989c214\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Requirement already satisfied: langchain_huggingface in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: langchain_milvus in /usr/local/lib/python3.11/dist-packages (0.1.10)\n",
            "Requirement already satisfied: docling in /usr/local/lib/python3.11/dist-packages (2.31.0)\n",
            "Requirement already satisfied: replicate in /usr/local/lib/python3.11/dist-packages (1.0.6)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from ibm-granite-community-utils==0.1.dev65) (1.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.56 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.59)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.24 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.24)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.40)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.9.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.39)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (3.4.1)\n",
            "Requirement already satisfied: pymilvus<3.0.0,>=2.5.5 in /usr/local/lib/python3.11/dist-packages (from langchain_milvus) (2.5.8)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from docling) (4.13.4)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.11/dist-packages (from docling) (2025.4.26)\n",
            "Requirement already satisfied: docling-core<3.0.0,>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from docling-core[chunking]<3.0.0,>=2.26.0->docling) (2.30.0)\n",
            "Requirement already satisfied: docling-ibm-models<4.0.0,>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from docling) (3.4.3)\n",
            "Requirement already satisfied: docling-parse<5.0.0,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from docling) (4.0.1)\n",
            "Requirement already satisfied: easyocr<2.0,>=1.7 in /usr/local/lib/python3.11/dist-packages (from docling) (1.7.2)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from docling) (1.2.0)\n",
            "Requirement already satisfied: lxml<6.0.0,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from docling) (5.4.0)\n",
            "Requirement already satisfied: marko<3.0.0,>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from docling) (2.1.3)\n",
            "Requirement already satisfied: openpyxl<4.0.0,>=3.1.5 in /usr/local/lib/python3.11/dist-packages (from docling) (3.1.5)\n",
            "Requirement already satisfied: pandas<3.0.0,>=2.1.4 in /usr/local/lib/python3.11/dist-packages (from docling) (2.2.2)\n",
            "Requirement already satisfied: pluggy<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from docling) (1.5.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from docling) (2.11.4)\n",
            "Requirement already satisfied: pylatexenc<3.0,>=2.10 in /usr/local/lib/python3.11/dist-packages (from docling) (2.10)\n",
            "Requirement already satisfied: pypdfium2<5.0.0,>=4.30.0 in /usr/local/lib/python3.11/dist-packages (from docling) (4.30.1)\n",
            "Requirement already satisfied: python-docx<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from docling) (1.1.2)\n",
            "Requirement already satisfied: python-pptx<2.0.0,>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from docling) (1.0.2)\n",
            "Requirement already satisfied: rtree<2.0.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from docling) (1.4.0)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from docling) (1.15.2)\n",
            "Requirement already satisfied: typer<0.16.0,>=0.12.5 in /usr/local/lib/python3.11/dist-packages (from docling) (0.15.3)\n",
            "Requirement already satisfied: httpx<1,>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from replicate) (0.28.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from replicate) (4.13.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (2.7)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: jsonref<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from docling-core<3.0.0,>=2.26.0->docling-core[chunking]<3.0.0,>=2.26.0->docling) (1.1.0)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.16.0 in /usr/local/lib/python3.11/dist-packages (from docling-core<3.0.0,>=2.26.0->docling-core[chunking]<3.0.0,>=2.26.0->docling) (4.23.0)\n",
            "Requirement already satisfied: latex2mathml<4.0.0,>=3.77.0 in /usr/local/lib/python3.11/dist-packages (from docling-core<3.0.0,>=2.26.0->docling-core[chunking]<3.0.0,>=2.26.0->docling) (3.78.0)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from docling-core<3.0.0,>=2.26.0->docling-core[chunking]<3.0.0,>=2.26.0->docling) (0.9.0)\n",
            "Requirement already satisfied: semchunk<3.0.0,>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from docling-core[chunking]<3.0.0,>=2.26.0->docling) (2.2.2)\n",
            "Requirement already satisfied: jsonlines<4.0.0,>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from docling-ibm-models<4.0.0,>=3.4.0->docling) (3.1.0)\n",
            "Requirement already satisfied: opencv-python-headless<5.0.0.0,>=4.6.0.66 in /usr/local/lib/python3.11/dist-packages (from docling-ibm-models<4.0.0,>=3.4.0->docling) (4.11.0.86)\n",
            "Requirement already satisfied: torch<3.0.0,>=2.2.2 in /usr/local/lib/python3.11/dist-packages (from docling-ibm-models<4.0.0,>=3.4.0->docling) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision<1,>=0 in /usr/local/lib/python3.11/dist-packages (from docling-ibm-models<4.0.0,>=3.4.0->docling) (0.21.0+cu124)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from easyocr<2.0,>=1.7->docling) (0.25.2)\n",
            "Requirement already satisfied: python-bidi in /usr/local/lib/python3.11/dist-packages (from easyocr<2.0,>=1.7->docling) (0.6.6)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.11/dist-packages (from easyocr<2.0,>=1.7->docling) (2.1.0)\n",
            "Requirement already satisfied: pyclipper in /usr/local/lib/python3.11/dist-packages (from easyocr<2.0,>=1.7->docling) (1.3.0.post6)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from easyocr<2.0,>=1.7->docling) (1.11.1.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.21.0->replicate) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.21.0->replicate) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.21.0->replicate) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.21.0->replicate) (0.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain_community) (0.3.8)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.56->langchain_community) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl<4.0.0,>=3.1.5->docling) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.1.4->docling) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.1.4->docling) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.1.4->docling) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->docling) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->docling) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->docling) (0.4.0)\n",
            "Requirement already satisfied: setuptools>69 in /usr/local/lib/python3.11/dist-packages (from pymilvus<3.0.0,>=2.5.5->langchain_milvus) (75.2.0)\n",
            "Requirement already satisfied: grpcio<=1.67.1,>=1.49.1 in /usr/local/lib/python3.11/dist-packages (from pymilvus<3.0.0,>=2.5.5->langchain_milvus) (1.67.1)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.11/dist-packages (from pymilvus<3.0.0,>=2.5.5->langchain_milvus) (5.29.4)\n",
            "Requirement already satisfied: ujson>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pymilvus<3.0.0,>=2.5.5->langchain_milvus) (5.10.0)\n",
            "Requirement already satisfied: milvus-lite>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from pymilvus<3.0.0,>=2.5.5->langchain_milvus) (2.4.12)\n",
            "Requirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from python-pptx<2.0.0,>=1.0.2->docling) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.6.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.16.0,>=0.12.5->docling) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.16.0,>=0.12.5->docling) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.16.0,>=0.12.5->docling) (13.9.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.56->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.26.0->docling-core[chunking]<3.0.0,>=2.26.0->docling) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.26.0->docling-core[chunking]<3.0.0,>=2.26.0->docling) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.26.0->docling-core[chunking]<3.0.0,>=2.26.0->docling) (0.24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.1.4->docling) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<0.16.0,>=0.12.5->docling) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<0.16.0,>=0.12.5->docling) (2.19.1)\n",
            "Requirement already satisfied: mpire[dill] in /usr/local/lib/python3.11/dist-packages (from semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.26.0->docling) (2.10.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.21.0->replicate) (1.3.1)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr<2.0,>=1.7->docling) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr<2.0,>=1.7->docling) (2025.3.30)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr<2.0,>=1.7->docling) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.6.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<0.16.0,>=0.12.5->docling) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (3.0.2)\n",
            "Requirement already satisfied: multiprocess>=0.70.15 in /usr/local/lib/python3.11/dist-packages (from mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.26.0->docling) (0.70.18)\n",
            "Requirement already satisfied: dill>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from multiprocess>=0.70.15->mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.26.0->docling) (0.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install \"git+https://github.com/ibm-granite-community/utils.git\" transformers pillow langchain_community langchain_huggingface langchain_milvus docling replicate\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "embeddings_model_path = \"ibm-granite/granite-embedding-30m-english\"\n",
        "\n",
        "embeddings_model = HuggingFaceEmbeddings(model_name=embeddings_model_path,)\n",
        "\n",
        "embeddings_tokenizer =AutoTokenizer.from_pretrained(embeddings_model_path)"
      ],
      "metadata": {
        "id": "-bqw221Xhit2"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ibm_granite_community.notebook_utils import get_env_var\n",
        "from langchain_community.llms import Replicate\n",
        "from transformers import AutoProcessor\n",
        "\n",
        "# Define the model path\n",
        "vision_model_path = \"ibm-granite/granite-vision-3.2-2b\"\n",
        "\n",
        "# Initialize the vision model\n",
        "vision_model = Replicate(\n",
        "    model=vision_model_path,\n",
        "    replicate_api_token=get_env_var(\"REPLICATE_API_TOKEN\"),\n",
        "    model_kwargs={\n",
        "        \"max_tokens\": 512,  # Adjust as needed or use a tokenizer to determine this dynamically\n",
        "        \"min_tokens\": 100,\n",
        "    }\n",
        ")\n",
        "\n",
        "# Initialize the processor for the vision model\n",
        "vision_processor = AutoProcessor.from_pretrained(vision_model_path)\n"
      ],
      "metadata": {
        "id": "x1gEb4iG6dQg"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''!pip install google-generativeai groq  torch accelerate'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fepOzXN9mttF",
        "outputId": "0298cec1-adda-426d-cad3-85eff8932076"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!pip install google-generativeai groq  torch accelerate'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''!pip install langchain-core langchain-google-genai'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dEHtovK4iWCC",
        "outputId": "8f6290ad-31d9-40c7-9461-6871fa236291"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!pip install langchain-core langchain-google-genai'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model path\n",
        "model_path = \"ibm-granite/granite-3.2-8b-instruct\"\n",
        "\n",
        "# Initialize the model with Replicate\n",
        "model = Replicate(\n",
        "    model=model_path,\n",
        "    replicate_api_token=get_env_var(\"REPLICATE_API_TOKEN\"),\n",
        "    model_kwargs={\n",
        "        \"max_tokens\": 1000,  # Maximum number of tokens in the output\n",
        "        \"min_tokens\": 100    # Minimum number of tokens in the output\n",
        "    }\n",
        ")\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
      ],
      "metadata": {
        "id": "vTkOxbwq76im"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gpx2oD_inF6P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "7886934d-84d3-40c2-90b6-751bb92a004f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport google.generativeai as genai\\nfrom groq import Groq\\nfrom PIL import Image\\nimport os\\nfrom langchain_google_genai import ChatGoogleGenerativeAI\\n# ---------------------------\\n# CONFIGURE API KEYS\\n# ---------------------------\\nGOOGLE_API_KEY = \"AIzaSyBTt66oOvxpLeYn41sR-KkjSYPK2vOAqkU\"\\nGROQ_API_KEY = \"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\"\\n\\n# ---------------------------\\n# SETUP GOOGLE GEMINI VISION\\n# ---------------------------\\ngenai.configure(api_key=GOOGLE_API_KEY)\\nvision_model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\\n\\n# ---------------------------\\n# LOAD AND PROCESS IMAGE\\n# ---------------------------\\nimage_path = \"/content/18.PNG\"  # Change to your image file\\nimage = Image.open(image_path)\\n\\n# ---------------------------\\n# GET IMAGE DESCRIPTION FROM GEMINI\\n# ---------------------------\\ngemini_response = vision_model.generate_content(\\n    [\"Give a detailed caption of this image.\", image],\\n    stream=False\\n)\\nimage_description = gemini_response.text\\nprint(\"Image Description:\\n\", image_description)\\n\\n# ---------------------------\\n# USE GROQ LLM TO ANALYZE IMAGE CAPTION OR ANSWER QUESTION\\n# ---------------------------\\nclient = Groq(api_key=GROQ_API_KEY)\\n\\nuser_question = f\"What kind of scene does this describe and what could be happening here?\\n\\nCaption: {image_description}\"\\n\\nresponse = client.chat.completions.create(\\n    model=\"mixtral-8x7b-32768\",  # Or use \"llama3-8b-8192\"\\n    messages=[\\n        {\"role\": \"user\", \"content\": user_question}\\n    ],\\n    temperature=0.7,\\n    max_tokens=800\\n)\\n\\nllm_output = response.choices[0].message.content\\nprint(\"\\nLLM Interpretation:\\n\", llm_output)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "eFOZ9uOlkIzB",
        "outputId": "e8833a42-1490-4aca-8410-f8b36383a190"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import google.generativeai as genai\\nfrom groq import Groq\\nfrom PIL import Image\\n\\n# ---------------------------\\n# CONFIGURE API KEYS\\n# ---------------------------\\nGOOGLE_API_KEY = \"AIzaSyBTt66oOvxpLeYn41sR-KkjSYPK2vOAqkU\"\\nGROQ_API_KEY = \"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\"\\n\\n# ---------------------------\\n# SETUP GOOGLE GEMINI VISION\\n# ---------------------------\\ngenai.configure(api_key=GOOGLE_API_KEY)\\nvision_model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")  # ✅ Correct class and method\\n\\n# ---------------------------\\n# LOAD AND PROCESS IMAGE\\n# ---------------------------\\nimage_path = \"/content/18.PNG\"  # Update your path here\\nimage = Image.open(image_path)\\n\\n# ---------------------------\\n# GET IMAGE DESCRIPTION FROM GEMINI\\n# ---------------------------\\ngemini_response = vision_model.generate_content(\\n    [\"Give a detailed caption of this image.\", image],\\n    stream=False\\n)\\nimage_description = gemini_response.text\\nprint(\"Image Description:\\n\", image_description)\\n\\n# ---------------------------\\n# USE GROQ LLM TO ANALYZE IMAGE CAPTION OR ANSWER QUESTION\\n# ---------------------------\\nclient = Groq(api_key=GROQ_API_KEY)\\n\\nuser_question = f\"What kind of scene does this describe and what could be happening here?\\n\\nCaption: {image_description}\"\\n\\nresponse = client.chat.completions.create(\\n    model=\"llama3-8b-8192\",  # ✅ Working Groq model\\n    messages=[{\"role\": \"user\", \"content\": user_question}],\\n    temperature=0.7,\\n    max_tokens=800\\n)\\n\\nllm_output = response.choices[0].message.content\\nprint(\"\\nLLM Interpretation:\\n\", llm_output)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
        "from docling.datamodel.base_models import InputFormat\n",
        "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
        "\n",
        "# Step 1: Define PDF processing options\n",
        "pdf_pipeline_options = PdfPipelineOptions(\n",
        "    do_ocr=False,  # Set to True if PDF is scanned (image-based)\n",
        "    generate_picture_images=True  # Whether to extract images from the PDF\n",
        ")\n",
        "\n",
        "# Step 2: Link input format to pipeline options\n",
        "format_options = {\n",
        "    InputFormat.PDF: PdfFormatOption(pipeline_options=pdf_pipeline_options),\n",
        "}\n",
        "\n",
        "# Step 3: Initialize the converter with format options\n",
        "converter = DocumentConverter(format_options=format_options)\n",
        "\n",
        "# Step 4: List of sources (can be file paths or URLs)\n",
        "sources = [\n",
        "    \"/content/P.S.Khade AI resume.pdf\"\n",
        "]\n",
        "\n",
        "# Step 5: Convert PDFs to structured documents\n",
        "conversions = {\n",
        "    source: converter.convert(source=source).document for source in sources\n",
        "}\n"
      ],
      "metadata": {
        "id": "eG8J5GZwsddn"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from docling_core.transforms.chunker.hybrid_chunker import HybridChunker\n",
        "from docling_core.types.doc.document import TableItem\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Initialize\n",
        "doc_id = 0\n",
        "texts: list[Document] = []\n",
        "\n",
        "# Process each converted document\n",
        "for source, docling_document in conversions.items():\n",
        "    chunker = HybridChunker(tokenizer=embeddings_tokenizer)\n",
        "\n",
        "    for chunk in chunker.chunk(docling_document):\n",
        "        items = chunk.meta.doc_items\n",
        "\n",
        "        # Skip if chunk is just a table\n",
        "        if len(items) == 1 and isinstance(items[0], TableItem):\n",
        "            continue\n",
        "\n",
        "        # Collect references from items\n",
        "        refs = \"\".join(item.get_ref().cref for item in items)\n",
        "        print(refs)\n",
        "        text = chunk.text\n",
        "\n",
        "        # Store as LangChain document\n",
        "        document = Document(\n",
        "            page_content=text,\n",
        "            metadata={\n",
        "                \"doc_id\": (doc_id:= doc_id + 1),\n",
        "                \"source\": source,\n",
        "                \"ref\": refs,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        texts.append(document)\n",
        "\n",
        "print(f\"{len(texts)} text document chunks created\")\n"
      ],
      "metadata": {
        "id": "VRQodo_Ntqyq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c7de3fc-48c8-42fa-e0bd-1a412f7084e5"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#/texts/1#/texts/4#/texts/5#/texts/6#/texts/7#/texts/8#/texts/9#/texts/10#/texts/11#/texts/12#/texts/13#/texts/14#/texts/15#/texts/16#/texts/17#/texts/18#/texts/19#/texts/20\n",
            "#/texts/22#/texts/23#/texts/24#/texts/25#/texts/26#/texts/27#/texts/28#/texts/29\n",
            "#/texts/31#/texts/32#/texts/33#/texts/34#/texts/35#/texts/36#/texts/37\n",
            "#/texts/39#/texts/40#/texts/41\n",
            "#/texts/44#/texts/45#/texts/46#/texts/47#/texts/48\n",
            "5 text document chunks created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from docling_core.types.doc.labels import DocItemLabel\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "doc_id = len(texts)  # Continue doc_id count from earlier\n",
        "tables: list[Document] = []\n",
        "\n",
        "for source, docling_document in conversions.items():\n",
        "    for table in docling_document.tables:\n",
        "        if table.label == DocItemLabel.TABLE:\n",
        "            ref = table.get_ref().cref\n",
        "            print(ref)\n",
        "            text = table.export_to_markdown()\n",
        "\n",
        "            document = Document(\n",
        "                page_content=text,\n",
        "                metadata={\n",
        "                    \"doc_id\": (doc_id:= doc_id + 1),\n",
        "                    \"source\": source,\n",
        "                    \"ref\": ref,\n",
        "                }\n",
        "            )\n",
        "\n",
        "            tables.append(document)\n",
        "\n",
        "print(f\"{len(tables)} table documents created\")\n"
      ],
      "metadata": {
        "id": "VeprlftHtuIv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e9f4b87-3fda-473d-826a-7d893cacc8fd"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 table documents created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import io\n",
        "import PIL.Image\n",
        "import PIL.ImageOps\n",
        "\n",
        "def encode_image(image: PIL.Image.Image, format: str = \"png\") -> str:\n",
        "    # Correct the typo and apply EXIF transposition if necessary\n",
        "    image = PIL.ImageOps.exif_transpose(image) or image\n",
        "\n",
        "    # Convert the image to RGB to avoid any issues with transparency\n",
        "    image = image.convert(\"RGB\")\n",
        "\n",
        "    # Save the image into a buffer in the specified format\n",
        "    buffer = io.BytesIO()\n",
        "    image.save(buffer, format=format)\n",
        "\n",
        "    # Encode the image buffer in base64 and return as a URI\n",
        "    encoding = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
        "    uri = f\"data:image/{format};base64,{encoding}\"\n",
        "\n",
        "    return uri\n",
        "\n",
        "# Image prompt to explain text in the image\n",
        "image_prompt = \"If the image contains text, explain the text in the image.\"\n",
        "\n",
        "# Define a conversation structure with user input\n",
        "conversation = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": image_prompt}\n",
        "        ]\n",
        "    },\n",
        "\n",
        "]\n",
        "\n",
        "# Assuming vision_processor is already defined and provides the method apply_chat_template\n",
        "vision_prompt = vision_processor.apply_chat_template(\n",
        "    conversation=conversation,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "# Initialize an empty list for pictures and document ID\n",
        "pictures:list[Document]=[]  # Initialize the pictures list\n",
        "doc_id = len(texts) + len(tables)\n",
        "\n",
        "# Iterate through the document sources and their pictures\n",
        "for source, docling_document in conversions.items():\n",
        "    for picture in docling_document.pictures:\n",
        "        ref = picture.get_ref().cref  # Assuming picture.get_ref().cref is valid\n",
        "        print(ref)\n",
        "\n",
        "        # Extract the image from the document\n",
        "        image = picture.get_image(docling_document)\n",
        "\n",
        "        if image:\n",
        "            # Process the image and apply vision model\n",
        "            text = vision_model.invoke(vision_prompt, image=encode_image(image))\n",
        "\n",
        "\n",
        "\n",
        "            # Create a document with the page content and metadata\n",
        "            document = Document(\n",
        "                page_content=text,\n",
        "                metadata={\n",
        "                    \"doc_id\": (doc_id := doc_id + 1),\n",
        "                    \"source\": source,\n",
        "                    \"ref\":ref, # Add image URI to metadata\n",
        "                },\n",
        "            )\n",
        "\n",
        "            # Append the document to your collection\n",
        "            pictures.append(document)\n"
      ],
      "metadata": {
        "id": "67vBXyiIvKn5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f24c457-c1ff-4191-85e5-f092cee1c752"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#/pictures/0\n",
            "#/pictures/1\n",
            "#/pictures/2\n",
            "#/pictures/3\n",
            "#/pictures/4\n",
            "#/pictures/5\n",
            "#/pictures/6\n",
            "#/pictures/7\n",
            "#/pictures/8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G_PJvDXpvMze",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "54eadf62-c8ca-4c07-a2cf-bd8e5ec9c008"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import base64\\nimport io\\nfrom PIL import Image, ImageOps\\n\\ndef encode_image(image: Image.Image, format: str = \"png\") -> str:\\n    # Corrected the `image` processing and EXIF orientation handling\\n    image = ImageOps.exif_transpose(image) or image\\n    image = image.convert(\"RGB\")\\n\\n    buffer = io.BytesIO()\\n    image.save(buffer, format=format)\\n\\n    encoding = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\\n    uri = f\"data:image/{format};base64,{encoding}\"\\n\\n    return uri\\n\\n# Prompt for explaining image text (if any)\\nimage_prompt = \"If the image contains text, explain the text in the image.\"\\n\\nconversation = [\\n    {\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": image_prompt}]},\\n    {\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": image_prompt}]}\\n]\\n\\nvision_prompt = vision_processor.apply_chat_template(\\n    conversation=conversation,\\n    add_generation_prompt=True,\\n)\\n\\npictures = []\\n\\ndoc_id = len(texts) + len(tables)\\n\\n# Corrected the document loop and structure\\nfor source, doc in conversions.items():\\n    for picture in doc.pictures:\\n        ref = picture.get_ref().cref\\n        print(ref)\\n\\n        image = picture.get_image(doc)\\n        if image:\\n            text = vision_model.invoke(vision_prompt, image)\\n            encoded_image = encode_image(image)\\n\\n            document = {\\n                \"page_content\": text,\\n                \"metadata\": {\\n                    \"doc_id\": doc_id := doc_id + 1,\\n                    \"source\": source,\\n                    \"ref\": ref\\n                }\\n            }\\n            pictures.append(document)\\n\\nprint(f\"{len(pictures)} image descriptions created\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "from docling_core.types.doc.document import RefItem\n",
        "from IPython.display import display\n",
        "\n",
        "# Assuming 'texts', 'tables', 'pictures', and 'conversions' are predefined lists or dictionaries\n",
        "\n",
        "# Print all created documents from texts and tables\n",
        "for document in itertools.chain(texts, tables):\n",
        "    print(f\"Document ID: {document.metadata['doc_id']}\")\n",
        "    print(f\"Source: {document.metadata['source']}\")\n",
        "    print(f\"Content:\\n{document.page_content}\")\n",
        "    print(\"=\" * 88)  # Separator for clarity\n",
        "\n",
        "# Print all created documents from pictures\n",
        "for document in pictures:\n",
        "    print(f\"Document ID: {document.metadata['doc_id']}\")\n",
        "    source = document.metadata['source']\n",
        "    print(f\"Source: {source}\")\n",
        "    print(f\"Content:\\n{document.page_content}\")\n",
        "\n",
        "    # Resolve reference and get image\n",
        "    docling_document = conversions.get(source)  # Use .get to avoid KeyError\n",
        "    if docling_document:\n",
        "        ref = document.metadata['ref']\n",
        "        picture = RefItem(cref=ref).resolve(docling_document)\n",
        "        image = picture.get_image(docling_document)\n",
        "\n",
        "        print(\"Image:\")\n",
        "        display(image)\n",
        "        print(\"=\" * 80)  # Separator for clarity\n"
      ],
      "metadata": {
        "id": "2_s7wsYgw9Bq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e7b32d7c-8472-4cf5-e0b3-05e022aec9eb"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document ID: 1\n",
            "Source: /content/P.S.Khade AI resume.pdf\n",
            "Content:\n",
            "prathamesh.khade20@pccoepune.org\n",
            "LinkedIn\n",
            "GitHub\n",
            "Visit to my portfolio website\n",
            "Pimpri ChinchwadCollege of Engineering\n",
            "Bachelor of Technology , Mechanical Engineering with Dual degree in Data science\n",
            "Pune, Maharashtra\n",
            "2020 - 2024\n",
            "Technical Skills\n",
            "Languages and Tools\n",
            "Libraries & Frameworks\n",
            ": Python, HTML, CSS, JS, SQL(MySQL), Tableau, Excel, Git, GitHub\n",
            ": Numpy,Pandas,Matplotlib,Seaborn, PySpark,Sk-Learn, Beautiful Soup, Tensorflow\n",
            "Data Science & Machine Learning :\n",
            "Data Collection, Data Preprocessing, Data Visualization, Data Warehousing, Linear\n",
            "and Logistic regression, KNN, Decision Tree, Random forest, SVM and K Means\n",
            "Mathmaticsfor ML & DL\n",
            ": Statistics, Probability, Matrices\n",
            "========================================================================================\n",
            "Document ID: 2\n",
            "Source: /content/P.S.Khade AI resume.pdf\n",
            "Content:\n",
            "Research Intern | Tata Motors - (\n",
            "Jun 2023 - Sep 2023\n",
            ")\n",
            " Automated Guided Vehicle : Designed and implemented an Automated Guided Vehicle (AGV), from conceptualization to implementation ,reducing human efforts by 20 % .\n",
            " Web app Development : Created a user-friendly Python application for the AGV's interface, allowing seamless control and monitoring of the vehicle's functions.\n",
            "Machine Learning intern | TE Connectivity - ( Nov2022 - Apr 2023 )\n",
            " Both projects encompassed data collection, preprocessing, model training, and evaluation, enhancing accuracy and security in diverse applications.\n",
            " Utilized the Dart library and Deep Learning techniques to achieve an R2 score of 0.85, resulting in a 5% increase in accuracy.\n",
            "========================================================================================\n",
            "Document ID: 3\n",
            "Source: /content/P.S.Khade AI resume.pdf\n",
            "Content:\n",
            "AI based Fashion productRecommenderSystem\n",
            " Developed a Generative AI-based Fashion Recommender System using a Kaggle dataset of  45,000 images .\n",
            " Conducted extensive data preprocessing and feature engineering, creating templates for each image and transforming  them into vectors using the techniques such as CNN and autoencoders .\n",
            " Leveraged cosine similarity , KNN to measure the likeness betweenImages and recommend the top 5 products.\n",
            " Used libraries such as NumPy, Torch, Scikit-Learn and Hugging Face to develop a Chabot to streamline the recommender system's  functionality and user interface, result in a 25% reduction in searching time.\n",
            "Bank Loan Analysis\n",
            "-  Created a bank loan analysis dashboard using Power BI, providing stakeholders with interactive visualizations and key performance indicators for loan portfolios.\n",
            "========================================================================================\n",
            "Document ID: 4\n",
            "Source: /content/P.S.Khade AI resume.pdf\n",
            "Content:\n",
            " Gate score (Data Science and AI) : 32.67\n",
            " TE connectivity AI cup global rank 8\n",
            "Publication:\n",
            "========================================================================================\n",
            "Document ID: 5\n",
            "Source: /content/P.S.Khade AI resume.pdf\n",
            "Content:\n",
            " Stanford University & DeepLearning.Ai Machine Learning Specialization.- MachineLearning\n",
            " Introduction to computer vision and image processing (IBM)- Computer vision\n",
            " DeepLearning.AI Generative Adversarial Networks(GANS)Specialization - GenerativeAI\n",
            " DeepLearning.AI Deep Learning Specialization - Deep learning\n",
            " IBM AI Engineering Specialization - IBM AI Engineering\n",
            "========================================================================================\n",
            "Document ID: 6\n",
            "Source: /content/P.S.Khade AI resume.pdf\n",
            "Content:\n",
            "The image does not contain any text.\n",
            "Image:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=17x16>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABEAAAAQCAIAAAB/UwMIAAABrklEQVR4AY1Su6rCUBBMNIqKogiBID5ALAQLCy0sbARTRdPmD6z8Cf0DfyLYipW1iCCoYNRGtBEbEUTE9yMOd0OEe4vcA1n27M7sztksq+t6v99XVXW/37Msy/wcu91Ozvv9BgA+rNfrVRSlUCgw0+k0l8sRwtImk8nz+cy0222zvCXH7XYfj0eOuvv9/mw2GwgEQPtbAsJQfjQagQCAjRDxeLxSqaRSKaR/nfv9Hg6Hq9VqJpN5Pp/gcPhwcBkOh6fTSdO0xWJBQbLRaJTn+fF4bAYNzuv1wjB2u10+n4e/Wq0IEYvFRFGcTCaHwwF1SZSNcrhA0mAwQD1ZltPpNOLoUCwWZ7MZ4rfbzXyn0QcICm2322AwWC6XBUFIJBKhUKjb7ZpZavDloA9A+FedTgf/FxygPR6PJEnX6xXjJQKswUGTSCQCHClBYj6fEwjzhVqO48yIwXG5XKi32WyAwGTpeaT28XhgbqVSqVarQcu3z3q9bjabTqcTYghKTWCBWy6XjUYDEow9bLVaZtrSwZpCiA1b4/P5LNEEuFwucDisWb1e7/V6/6FBucPh+ACqP989oFqOQwAAAABJRU5ErkJggg==\n",
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAQABEDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDgteklvdZ1m9utW1SW6/tC4UQQSYIjR8ZGTzj0HQVmwHfp81/b6vq9vPEhmtxLN/rNpGSMHIA9a09WkNjrerQzabq0V8mo3LJcwRA4RnJwMjjPr6VQkvEewuYm0/WLm5eIxQSzQgeWCRkcDkcdOxoA+wN7f3jRSYPoaKAP/9k=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Document ID: 7\n",
            "Source: /content/P.S.Khade AI resume.pdf\n",
            "Content:\n",
            "The text in the image is incomplete and does not provide enough information to determine its exact content. It appears to be a placeholder or a section that is meant to be filled in with relevant information. Without additional context or visible text, it is not possible to accurately describe what the text says. To provide a detailed explanation, more information or a clearer image would be required.\n",
            "Image:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x18>"
            ],
            "image/png": "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\n",
            "image/jpeg": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Document ID: 8\n",
            "Source: /content/P.S.Khade AI resume.pdf\n",
            "Content:\n",
            "The image contains text, specifically the word \"python\". It is written in a stylized font that is commonly associated with the Python programming language. The text is placed centrally and is the most prominent feature of the image. The font is sans-serif, which is often used for its readability and modern appearance. There are no other discernible texts or elements in the image.\n",
            "Image:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=18x19>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABIAAAATCAIAAAAS8MqlAAACq0lEQVR4AY1TPUiqYRTWL6+CQlik0sVq0XAoSwsbxIaGGlQoQxRHcXCQBquhSQgEaSiEdBCEBpfCwSCcKwolCIlSkBQURcufEoobVFrP53sxBy/cM7y83znPc85z3nM+5tfXF6OXNZvNvr6+XhHax+oOfHx8pFKpYrF4c3OTzWZHR0enp6eHh4dxstnsbiSzUy2TyYRCoVgs9rttFEUBVy6X7+/v5+fnDQaDQqH4YYLWarUuLy8tFovZbA6Hw9VqFU5iT09P0WjUarXq9frT01MoJ35aJPT4fL7BwcGVlZU/bWs0Gp+fnygIg9Tl5WWo2N3d5fP5U1NToDDf3t48Hg+UuN1uiNzZ2ZmYmOjv739+fsYJRCKR0Ol0+/v7GxsbeCSv18tisRjofnFxMRKJvL+/OxwOpKeTMZnkJBeogLazszOtVnt1dYU7VSgU8FZqtRrpLy4u4AKhc5LL3d0dQnK5nMfjHR4eAkChmlgsHhoaqlQqaJLgEOi2fD4PJpfLlUgkuVwOIQpQooTD4fxrvmjmV9ugi2CokZERUkEgEKhUqu4inbtMJpubm8MyQBFmQFfDEEulUq1Wwx6sr6+jafCRGjEkHhgYwIpsb2/jeV9fX7ESY2NjCLGwEtCJcUMketjb20smk5ubm0BgSi6Xa2lpSSqVAgoY5rm2tkZXw/toNJqTkxOoTafTx8fH5+fnWA7EAAJzfHwczb+8vBwcHIhEopmZGZqGIqurq4+Pj5i1yWTCc9XrdawIYlg6DBMXcPx+P3qx2+1EPz1ctOd0OqEtGAyi+MLCAuYDPwwFr6+vt7a24vG4zWYjpeD/+wcg8e3tbSAQQHsPDw+AtlmMyclJ7KRQKEQdpVJJr1Xbfn4cfEIbNvDo6IjMFB5wjEbj7Oxsh9CDRlz/c34DxeVtS1aKzy8AAAAASUVORK5CYII=\n",
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAATABIDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDqfGPxDTSp4bi+ur2y0WW4ktovsKr585ThnJbomeBjmqHhv4mW2oX850K91S9tLO3+0XdtqYQuYwcMY2HJYA5IPGBWD8RtBhv/AA6NOu/PgvdAFw6Mg3+bEzblJBwdpGPmGcEEEVnfCvw5Zaf4Yn8Si6NxqupRS2FlYKozuJ2knnkd88ADNAH0jDLHcQxzROHjkUOjDoQRkGiq+lWZ07SLKxL7zbQRw7vXaoGf0ooAr6z4e0jxDbfZ9W0+C7jxgeYvI+hHIqDQ/COgeGwRo+lW9oTnLKCW+mTk49qKKANqiiigD//Z\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Document ID: 9\n",
            "Source: /content/P.S.Khade AI resume.pdf\n",
            "Content:\n",
            "The image contains the text \"in,\" which is the first letter of the word \"in.\" This could be part of a larger word or phrase that is not fully visible in the image provided. Without additional context or a clearer image, it is not possible to determine the full text or the intended message behind the word \"in.\"\n",
            "Image:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=16x16>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAIAAACQkWg2AAABsUlEQVR4AYVSu64BURQ9M0a4iHgUNF5Rikp0tzCViKj0Igql39AqfIJGxB8ohV4jJF6JVzwSb/GauYs9yZW4rp3MmTPrnDVr77U31+v1qtXqZrNhjHEch/U1ZFnmed7pdIbDYZbJZPR6vVar1Wg0WL/eh81m2+12wnA43O/3ZrM5Eoms1+tKpXI6nV5FgEAfUiwej+MjGo2uVqt2ux0IBP68DdBgMGy3W4GOoTUYDCaTyfF4fEcgXKBCm81mNptFSuA4HA6TyTSdTo1Go8fjmc1mOL1cLgqBXl6vN51Od7vd5XIZi8VCoVC/37dYLD6fbzwe5/P5YrF4L4AxJSWr1SqKot1uL5fLfr//+xHn81mlUrlcLrhSr9ehCQJPCsS+Xq/Y3G43gIVCIRgMJhIJSZLcbjeagM2vAtGe106n02g0QEZQl+inisLz1f/3CoG8EoS7acgbHMwCVnyq1WrCHyCnFI2yYBGcORwOsAU9gV24gbrRzcViARxkSZIFKgXpJpNJoJjF+XxeKpXAB2E0GqVSKZjRarVImeVyOYji7GNgLDEQHJ5arYYefyTodDqM9w8ZrOUfksPmugAAAABJRU5ErkJggg==\n",
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAQABADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDkhY6/rmhG50641yXVrnV5bZJftmIWUAnbjOc07StE8e+GvF/hefXZdRhtbrU4IvnuiwP7wZVhnuM8Gt/w1rlz4ZtrXT73w9rbTWeuzXkhismZShUqMHuc1D4e1C+uYdL0iXRdZWd/Fkeo+bLaOESEuvUkcYwaAP/Z\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Document ID: 10\n",
            "Source: /content/P.S.Khade AI resume.pdf\n",
            "Content:\n",
            "The image contains text, but due to the low resolution and blurriness, it is not possible to accurately transcribe or interpret the text. The text appears to be in a standard font, which could suggest that it is meant to be legible and readable. However, without clearer visual information, any attempt to describe the text would be speculative and not based on a reliable analysis of the image's content.\n",
            "Image:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=18x19>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABIAAAATCAIAAAAS8MqlAAAC6klEQVR4AZWTS0uyURSFP+slu2sXosAILDIsKJNu1kAKAptIogRCDWwSgSODcNQvaNRMHZTQqKIoolEgdBt0hSAsrUwlKjLtbln2PS/9gvZAtuectfdae+1X8vPz8+/vIQAB+RuZmZn83d/fD4fDqVQqIyOD87y8PJ1OJ5fLuUqn0xySSMh+4/v7e3d3d3t7e3Nz8/LyMhKJtLa2Xl1dNTQ0qNVqjUbT29ubk5MjkUhAijDQ1Pb5fAsLCy0tLXd3d9fX15SvqanJz8+nSltb28bGRllZmdVqVSgUgEUaBKfT09Nms1mv1z89PQ0MDFRXVz8/PwOg/NfX18TExM3NzeTk5Nvbm9gQbsFg0O12Pzw8nJ2dzc3NlZSU0BNJpaWllZWVFovl8PAQLq+vr7BYX18XtdEKTDwe7+rqQtjS0lJFRUVTU9PBwQEVOzo67u/v4UmtoaGhgoICcofDIaAKMY2NjYzr8/OT1+Bzc3P39vbgI5VK6fz+/l5fX28ymaC9trZGdYFxn5ycGI1GWkOyrq5udHSUHD0fHx/kKKEK5Lu7u1UqFQ0fHx8FLNra2lpcXKQ7Amju8XgA7OzsACgsLOTk4uLi6OjI5XLhxPHxsVarFdCGhkQiwaOXlxdyJoYzDIAr3KMtyumPyNvbWyqSC7AyGAwYQg3qxWIxp9PJBX5QYnh4mHx5eZme4+PjuELCCAS0FhcXQwOYUqnEwPn5eeZ+enoKANrZ2dlsD4MBQ0N4sQMCwevz83O/34/R8JmamqqtrQ0EAtDmEczRyZzZBJaON83NzaJvTGVsbAw9RUVFWVlZMpkMZ1i0ZDLJJBnVzMxMeXk5mrltb28fHBwU1xl/bTYbnnBkt9sRjSV8DZAJhUJer5dtHhkZQTar09fXB0SCbqryiw2rq6s4Q38WH+mQp/zKykp/fz9zhwuLUlVVJX4BXDNiAk9ZudnZWQbFa1ans7OTwbAr0Wi0p6eH5YQCPWAhaqPpX+M/Nn/vR5rLNPsAAAAASUVORK5CYII=\n",
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAATABIDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDvvFPie/N29lZz3Gn2xmFst7HCJC8gPz467Qoz25rF0bxPq8D25s9VvNckuIyywSw8MUYhxu2qF+XDZ57jHSo57Y6RrcNoLnWZbm2vp2eOKUIFjmJ2OGYbQCWx1qpbyeQNEF/PrdpDbGW4eVbiOTYCWReFyV3McZI5oA9Zttb065tYZ1u4gJEDgMwBGRnmiuP07wlqP9mWm4Rq3kpkSH5gdo6+9FAHY6joem6qF+22iSlejZKnHpkEHHt0qrbeEdCtLqO5h09BLGAFLOzAY6cEkZHPPuaKKANrAFFFFAH/2Q==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Document ID: 11\n",
            "Source: /content/P.S.Khade AI resume.pdf\n",
            "Content:\n",
            "Since the image is blurred, it is not possible to accurately determine the content or any text it might contain. The lack of clear visual information prevents any meaningful interpretation or explanation of the text. To provide a detailed answer, a clearer image would be required.\n",
            "Image:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=15x14>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAA8AAAAOCAIAAAB/6NG4AAAB1klEQVR4AV2ROY+BURSG57u+GI1YCluDBLFEiCVaBRUKBSIKKp1OIX6An+A/SISIiCVaCRJBLA0JhVBZEgpLMK/5JjIzt7i595znvuec91LP5/Pj77pcLtfrFfHP7/U7Sb8v9/t9Nps1Go3pdLrdbvFGJBKZTCa3261Wq1ksFkiK0T4ej8VisdlsyuVyg8GgUCiQWy6XeInd5XL5/X4ul/vSRt18Pl+tVh0Oh9FoBI0EVDQaDZ61Wq1KpYLKkUjkRY/H41qtFgwGN5tNMpmUSCRsNhsS2BEJBALhcDiXy5nNZhoa9XpdpVJ5PJ50Oj2ZTEajESTey2azJRIJVECf5Hw+QxsNrNfrdrv9eDzeHHPo9XqLxcJqtQ4GA3K73fb7PaoTQpiJ/9EA0DSAw+EAhsDW0+kEvzAlzr9pGGexWJRKJQCkaA6Ho9Pp4BT2UCgEjVKp1Ol08AZmY5h4PA6LMIxWqyU0TTudzuFwiHu32xWLxYAoigLt9XpTqRR+ANl+vw+MIIoG4An8lkqlu91utVoxAzATo2w2m4UwsBfN4/Gi0Sh2WCkQCGQyGYJY8/m8UChkMhk+nx+LxQD8/DxycKZcLsMvpitE9Ho9urLb7T6fTygUIvIFnp7yKLZUGIsAAAAASUVORK5CYII=\n",
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAOAA8DASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDe+IXxEl0OeO4m+1NpzXUtrHb2k5hdzHgO7OBnG44AGOmar+C/ih9tt7zUkF2ul2siRTW13OZ5E3A7XVyM43DBBz1zTfjBoy2Oi33mLDNaXDyXFsrLloJmxux6AnnIPc8Gj4XaHZa74Qs7COzgtrOMCW/ZcmS7lwQufRRnP1oA/9k=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Document ID: 12\n",
            "Source: /content/P.S.Khade AI resume.pdf\n",
            "Content:\n",
            "The image provided is too blurred to discern any text. If you have another image or if there's a specific text you're referring to, please provide it, and I'll be happy to help explain it.\n",
            "Image:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=14x15>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAA4AAAAPCAIAAABbdmkjAAAB3ElEQVR4AV1SS8txYRR1HqeUciulXBIhlEtiRDE0kGQiGRH/wdhEGZmZmxgZmigUJsolCSHlktsIKYkP3/Kd9/16e/fgnPZ+1l7P2ns91Pv9Zv2I5/N5u93wpWmay+X+OGHR/5Pj8dhoNOr1+m63u1wuPB7PYDB4vV6TycThcACjGNbxeJzP54G2WCxAgO9wOAyHw8lkYrfbI5GIUCj8sG42m1wuh0vdbrfZbJbL5Sjq9Xq1Wi0QCJrNJtJ4PP6BFovF1+sVCASy2SzIAL1eryBerVYejyccDhcKBYfDQe/3+3a7HY1GccVoNFosFp1OB/1MyGSyZDLZ7XYrlQqZzWYURUkkEuSg+cZ8/dEMuVarFbrJ6XRis9nQhEPI+AX98y9AdD6fCRaBgQAyGo0ajeYXFEWtVgvpgBGMCQHz+RxzpNPpUCiEjaJBoVAkEglUIHcwGICFqFQqjFyr1abT6Xa7xb74fD6gNpstlUpBJVharZbL5SIQGgwGkS+XS4DgBYwFlPF2vV5j5VKp1Ol0ElTRinv7/T6WqtPpGBuxxHK5nMlkHo9HLBYTiUQfCwghPp8PraVSqdfr4aFgx+CuVquQ4ff7Gf++3gAamIAM+Hy/3zGcUqkUi8XfJ6y/UDLtcSFENlUAAAAASUVORK5CYII=\n",
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAPAA4DASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDV8e/EptFvreW6W8lsLh5kgt7O5Nu2I22l2cDJyc8Z4Aq14Q+J4k0ZtVn+0NpjzNbpFcS+bLC4AI+fA3Agnr0xUPxQ8B6nfaRcw2FnDeReaZbQlkV4C7bnXLEcZz0z7jvW54a8CJc6BYWV/pkOnWFvCCLeNwzyzkDdKxGR2IAz3oA//9k=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Document ID: 13\n",
            "Source: /content/P.S.Khade AI resume.pdf\n",
            "Content:\n",
            "The image contains text, which appears to be a play button. The play button is a universally recognized symbol used to start or pause the playback of audio or video content. It is typically depicted as a white triangle with a black border, and within it is a white circle representing the play or pause action. The text within the play button is not legible due to the image's resolution and the angle at which the button is presented.\n",
            "Image:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=15x11>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAA8AAAALCAIAAAAvJUALAAABf0lEQVR4AY2QzS4EQRDHu7tqzM7a7E6Wk8VViISrj3gBR+Lk4ObRcPIMEglLXBAfB+IjiA1218zamZ3uKtWeQKe7k67+V9Wv/pqZ1b8X/inZJWnx8uw6X5T1FbGSEtr/6CCAahyMT0Bc1wBenV+ed/f37Ocn9RIuBmztXwkv12B0qWziOJyaGdnaRmtt9+YqO2sqrf0m1pVhGhSq/yNP4WT+4Ic7lWdkrcne39qnRwygDEgtVjS0sBitrqowVM4pYwTAIDpHvU4bgzSJXp88q3Rm8rWr1fLauqnV7PX14KTJkqNU/7tr7++Mh5PjnfH2+M7EEIa4uJSWh9MsI0eCZNPk5/kRReXYAyrvpL9dL01vb1u7O+lxM3CWgCKZ1YAOI3SVWt6YDG8uBFmk2pjvg4Pk8NC1WkhkACRcODtUr8dz81gaGxtdXknOzyQqlghKKe/bLM+Y0RhkAhmdLGJQbzQQtK5Mz6qNzWJQOOlLJEA1yREXWKHWgBAAROOTAvMLxZfTMSPw4joAAAAASUVORK5CYII=\n",
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAALAA8DASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDsbvxLrN/4pk0+2vvssRnaJMAYXHr+VJoXizVIfEQs7++W5gBZX2YYEgHGCPel8R6VZNrl25gwWfJwxHOPY0/whpVlDr8MiQAMFbGWJ/hPqa4Ep8+/U+jk8P7B2j9nst7b3P/Z\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Document ID: 14\n",
            "Source: /content/P.S.Khade AI resume.pdf\n",
            "Content:\n",
            "The image contains text that reads \"QWERTYUIOP\". This is a common layout of letters on a keyboard, designed for ease of typing and to prevent jamming. The letters are arranged in a grid pattern, with the most frequently used letters (Q, E, R, T, Y, U, I, O, P) placed at the top row for quick access. This layout is a standard in English-language keyboards and is widely recognized around the world.\n",
            "Image:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=14x14>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAIAAACQKrqGAAAB3ElEQVR4AU1Su8uBcRT+3tf9Um7JbRAmSqQkAwPKpLAaFWXzLxiZDTYDo5QyKCxKJJcSGSiT3HIp19y+x/cu3284vb9znvOc8zy/l/h8Pj9/53w+N5vNwWCwXC5ZLJbRaHQ6nSqViqoiEoA+n89Go1EsFk+nk06nk0gkaJvNZogej8fn8wmFQkDpgNbr9Ww2q9frw+GwVqvlcrmv12u/3yNfKpW22200GkWSmE6niUTCbDajjOkmk+n9ftNotMPhgCEWi6VWq/n9/kAg8JPJZOLx+GKxAOV3Cp3+XYsgENVqdb/fz+fzkUjkcrmQnU7HarWCo9froYy9ESmt0DccDrEYlh6Px+Rms0E3oMfjEaD/536/Y5pAIIAhq9WKRA0ccrlcoVD8x+GbzWZrNBrgsD2upEwmm8/nfD4/FotBBJQiC1lKpRLCXS4XrHg8HoCRdrsd63a7XXCnUinYDigmpNPpZDIJj1utFnw1GAykw+HY7Xaj0Qgay+UylqNYxWIxk8nE01SrVbfbzeFwSHgeCoVgCpShTSqVAnq73SC/UqnASpvN5vV6kaTDQjQxGIxCoTCZTMCKzPV6zeVyIpEIpWAwSAn4/gPowMEC4G632+v1msfj4dnABzVUFfEXA2H7HNxywRwAAAAASUVORK5CYII=\n",
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAOAA4DASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwCHxz8Rp9K1KzmvtOXU47+3+0xQTTPHHAhYhQoU4JwMknua7H4ceM5rzSLa7uhILO8SVo4SxcwNG4UqrMclSGB56EGsL4p+CLs6ZFBby2slsLgG287holySU+6eOexHHUcV6P4Z0EpHBe3cNpGi2wgtbS2X93CnBY8gZYkAnjtQB//Z\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tempfile\n",
        "from langchain_core.vectorstores import VectorStore\n",
        "from langchain_milvus import Milvus\n",
        "\n",
        "# Create a temporary file for the database\n",
        "db_file = tempfile.NamedTemporaryFile(prefix=\"vectorstore\", suffix=\".db\", delete=False).name\n",
        "\n",
        "print(f\"The vector database will be saved to {db_file}\")\n",
        "\n",
        "# Initialize the vector store\n",
        "vector_db: VectorStore = Milvus(\n",
        "    embedding_function=embeddings_model,  # Ensure 'embeddings_model' is defined\n",
        "    connection_args={\"uri\": db_file},\n",
        "    auto_id=True,\n",
        "    enable_dynamic_field=True,\n",
        "    index_params={\"index_type\": \"AUTOINDEX\"}  # Corrected the index_params syntax\n",
        ")\n"
      ],
      "metadata": {
        "id": "p9IzEqghw-Ok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1769e97-cac6-4e7d-cba2-41a7f2e37eaf"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vector database will be saved to /tmp/vectorstoreiba5w9nu.db\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "# Combine all the document types into a single list\n",
        "documents = list(itertools.chain(texts, tables, pictures))\n",
        "\n",
        "# Add documents to the vector database\n",
        "ids = vector_db.add_documents(documents)\n",
        "\n",
        "# Print the number of documents added\n",
        "print(f\"{len(ids)} documents added to the vector database\")\n"
      ],
      "metadata": {
        "id": "DMRPsF0QyTT1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33ad5fa3-dcde-41de-c532-37095bb20dcd"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14 documents added to the vector database\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is this pdf is about ?...which skills the person have\"\n",
        "\n",
        "for doc in vector_db.as_retriever().invoke(query):\n",
        "    print(doc)\n",
        "    print(\"=\" * 80)\n"
      ],
      "metadata": {
        "id": "IVsOT77PyTOw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7afff51c-e2e5-40e4-fb7a-c334938fd766"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='prathamesh.khade20@pccoepune.org\n",
            "LinkedIn\n",
            "GitHub\n",
            "Visit to my portfolio website\n",
            "Pimpri ChinchwadCollege of Engineering\n",
            "Bachelor of Technology , Mechanical Engineering with Dual degree in Data science\n",
            "Pune, Maharashtra\n",
            "2020 - 2024\n",
            "Technical Skills\n",
            "Languages and Tools\n",
            "Libraries & Frameworks\n",
            ": Python, HTML, CSS, JS, SQL(MySQL), Tableau, Excel, Git, GitHub\n",
            ": Numpy,Pandas,Matplotlib,Seaborn, PySpark,Sk-Learn, Beautiful Soup, Tensorflow\n",
            "Data Science & Machine Learning :\n",
            "Data Collection, Data Preprocessing, Data Visualization, Data Warehousing, Linear\n",
            "and Logistic regression, KNN, Decision Tree, Random forest, SVM and K Means\n",
            "Mathmaticsfor ML & DL\n",
            ": Statistics, Probability, Matrices' metadata={'doc_id': 1, 'source': '/content/P.S.Khade AI resume.pdf', 'ref': '#/texts/1#/texts/4#/texts/5#/texts/6#/texts/7#/texts/8#/texts/9#/texts/10#/texts/11#/texts/12#/texts/13#/texts/14#/texts/15#/texts/16#/texts/17#/texts/18#/texts/19#/texts/20', 'pk': 457916610033221632}\n",
            "================================================================================\n",
            "page_content='The text in the image is incomplete and does not provide enough information to determine its exact content. It appears to be a placeholder or a section that is meant to be filled in with relevant information. Without additional context or visible text, it is not possible to accurately describe what the text says. To provide a detailed explanation, more information or a clearer image would be required.' metadata={'doc_id': 7, 'source': '/content/P.S.Khade AI resume.pdf', 'ref': '#/pictures/1', 'pk': 457916610033221638}\n",
            "================================================================================\n",
            "page_content=' Stanford University & DeepLearning.Ai Machine Learning Specialization.- MachineLearning\n",
            " Introduction to computer vision and image processing (IBM)- Computer vision\n",
            " DeepLearning.AI Generative Adversarial Networks(GANS)Specialization - GenerativeAI\n",
            " DeepLearning.AI Deep Learning Specialization - Deep learning\n",
            " IBM AI Engineering Specialization - IBM AI Engineering' metadata={'doc_id': 5, 'source': '/content/P.S.Khade AI resume.pdf', 'ref': '#/texts/44#/texts/45#/texts/46#/texts/47#/texts/48', 'pk': 457916610033221636}\n",
            "================================================================================\n",
            "page_content=' Gate score (Data Science and AI) : 32.67\n",
            " TE connectivity AI cup global rank 8\n",
            "Publication:' metadata={'doc_id': 4, 'source': '/content/P.S.Khade AI resume.pdf', 'ref': '#/texts/39#/texts/40#/texts/41', 'pk': 457916610033221635}\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.retrieval import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    conversation=[{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"(input)\",\n",
        "    }],\n",
        "    add_generation_prompt=True,\n",
        "    tokenize=False,\n",
        ")\n",
        "\n",
        "# Append the required variable \"context\" to fix the error\n",
        "prompt_template = PromptTemplate.from_template(template=prompt + \"\\nContext: {context}\")\n",
        "\n",
        "document_prompt_template = PromptTemplate.from_template(template=\"\"\"\\\n",
        "Document {doc_id}\n",
        "{page_content}\"\"\")\n",
        "\n",
        "document_separator = \"\\n\\n\"\n",
        "\n",
        "combine_docs_chain = create_stuff_documents_chain(\n",
        "    llm=model,\n",
        "    prompt=prompt_template,\n",
        "    document_prompt=document_prompt_template,\n",
        "    document_separator=document_separator,\n",
        ")\n",
        "\n",
        "rag_chain = create_retrieval_chain(\n",
        "    retriever=vector_db.as_retriever(),\n",
        "    combine_docs_chain=combine_docs_chain,\n",
        ")\n"
      ],
      "metadata": {
        "id": "JOTyiVSdIUEw"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = rag_chain.invoke({\"input\": query})\n",
        "print(outputs[\"answer\"])\n"
      ],
      "metadata": {
        "id": "LkedvInN1Q0S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73270822-cd7e-41d1-814b-eadb38a79d11"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided documents:\n",
            "\n",
            "1. Prathamesh Khade is a student at Pimpri Chinchwad College of Engineering, pursuing a Bachelor of Technology degree in Mechanical Engineering with a dual degree in Data Science. His email is prathamesh.khade20@pccoepune.org, and he maintains a presence on LinkedIn and GitHub. You can visit his portfolio website for more information.\n",
            "\n",
            "2. Prathamesh's technical skills include proficiency in various programming languages and tools such as Python, HTML, CSS, JS, SQL (MySQL), Tableau, Excel, Git, and GitHub. He is also experienced with data science libraries and frameworks like Numpy, Pandas, Matplotlib, Seaborn, PySpark, Scikit-learn, and Beautiful Soup, as well as deep learning frameworks such as TensorFlow.\n",
            "\n",
            "3. His expertise lies in data science and machine learning, covering areas such as data collection, data preprocessing, data visualization, data warehousing, and various machine learning algorithms including Linear and Logistic Regression, KNN, Decision Trees, Random Forest, SVM, and K-Means. He also has a strong foundation in mathematics for Machine Learning and Deep Learning, including statistics and probability, and matrix mathematics.\n",
            "\n",
            "4. Prathamesh has completed several online specializations related to machine learning, deep learning, and computer vision. These include the Stanford University & DeepLearning.Ai Machine Learning Specialization, an Introduction to Computer Vision and Image Processing course from IBM, the DeepLearning.AI Generative Adversarial Networks (GANs) Specialization, the DeepLearning.AI Deep Learning Specialization, and the IBM AI Engineering Specialization.\n",
            "\n",
            "5. Prathamesh's academic achievements include a Gate score of 32.67 in Data Science and AI, and he was ranked 8th globally in the TE Connectivity AI Cup.\n",
            "\n",
            "6. There is a publication mentioned, but without additional context, it's unclear what this publication entails.\n",
            "\n",
            "7. An image in Document 7 appears to be a placeholder or section intended for additional information, but it's too incomplete to decipher its content.\n"
          ]
        }
      ]
    }
  ]
}